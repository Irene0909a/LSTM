{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wkiQm50EvwL"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import tensorflow as tf \n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Bidirectional,GRU\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edbJ6tsYEzfk"
      },
      "outputs": [],
      "source": [
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return np.sqrt(np.mean(np.square(y_pred - y_true)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D262Jx65Ez_z"
      },
      "outputs": [],
      "source": [
        "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, single_step=False):\n",
        "  data = []\n",
        "  labels = []\n",
        "  \n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "  \n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    data.append(dataset[indices])\n",
        "    \n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "  \n",
        "  return np.array(data), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ79JC5WFrU3"
      },
      "outputs": [],
      "source": [
        "##GRU\n",
        "def gru_model(input_length, input_dim):#input_length所選取天數;input_dim所選取特徵數\n",
        "\n",
        "    d=0.3\n",
        "    #return_sequences : True 為維持 (batch ,time_steps ,seq_len) ，連接下一層LSTM 設置 False 將會變成一維\n",
        "    model= Sequential()\n",
        "    model.add(GRU(256,input_shape=(input_length, input_dim),return_sequences=True))\n",
        "    model.add(Dropout(d))\n",
        "    model.add(Dropout(d))\n",
        "    model.add(GRU(256,input_shape=(input_length, input_dim),return_sequences=True))\n",
        "    model.add(Dropout(d))\n",
        "    model.add(GRU(256,input_shape=(input_length, input_dim),return_sequences=False))\n",
        "    model.add(Dropout(d))\n",
        "    \n",
        " \n",
        "    model.add(Dense(1,activation='linear',kernel_initializer=\"uniform\"))#linear / softmax(多分類) / sigmoid(二分法)\n",
        "\n",
        "    # optimizer = tf.keras.optimizers.Adam(lr=0.00005)\n",
        "    model.compile(loss='mse',optimizer='adam', metrics=['mean_squared_error'])#loss=mse/categorical_crossentropy\n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "9H8K6_S4Ir7u",
        "outputId": "5a3722a9-04d0-4884-d9bb-5553c92660fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "Mounted at /content/gdrive/\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Close  Average Confirmation Time  Block Height    Difficulty  \\\n",
              "Date                                                                          \n",
              "2021-06-29  41564.36                     168.03      687456.0  1.990000e+13   \n",
              "2021-06-30  43790.89                     168.03      687456.0  1.990000e+13   \n",
              "2021-07-01  48116.94                     168.03      687456.0  1.990000e+13   \n",
              "2021-07-02  47711.49                     191.55      687456.0  1.990000e+13   \n",
              "2021-07-03  48199.95                     191.55      689472.0  1.990000e+13   \n",
              "...              ...                        ...           ...           ...   \n",
              "2022-06-26  19225.70                      28.97      741888.0  2.960000e+13   \n",
              "2022-06-27  19081.00                     128.29      741888.0  2.960000e+13   \n",
              "2022-06-28  19411.00                     128.29      741888.0  2.960000e+13   \n",
              "2022-06-29  19593.00                     128.29      741888.0  2.960000e+13   \n",
              "2022-06-30  19423.00                      19.40      741888.0  2.960000e+13   \n",
              "\n",
              "            Miners Revenue (USD)  Unique Addresses Used  Blockchain Size (MB)  \\\n",
              "Date                                                                            \n",
              "2021-06-29           20581265.56               379066.0             352241.73   \n",
              "2021-06-30           20581265.56               567655.0             352241.73   \n",
              "2021-07-01           22482193.60               567655.0             352608.29   \n",
              "2021-07-02           22482193.60               567655.0             352608.29   \n",
              "2021-07-03           22482193.60               583136.0             352608.29   \n",
              "...                          ...                    ...                   ...   \n",
              "2022-06-26           18725062.51               557791.0             413407.51   \n",
              "2022-06-27           18725062.51               557791.0             413407.51   \n",
              "2022-06-28           18725062.51               624229.0             413407.51   \n",
              "2022-06-29           19294230.34               624229.0             413947.56   \n",
              "2022-06-30           19294230.34               624229.0             413947.56   \n",
              "\n",
              "            Output Value Per Day        Volume       RPI  TOTRESNS  NONREVSL  \\\n",
              "Date                                                                           \n",
              "2021-06-29            1376609.20  3.790146e+10  17827.22    3848.1   3308.21   \n",
              "2021-06-30            1376609.20  3.405904e+10  17827.22    3848.1   3308.21   \n",
              "2021-07-01            1376609.20  3.783896e+10  17985.18    3943.9   3318.74   \n",
              "2021-07-02            1732809.50  3.872897e+10  17985.18    3943.9   3318.74   \n",
              "2021-07-03            1732809.50  2.438396e+10  17985.18    3943.9   3318.74   \n",
              "...                          ...           ...       ...       ...       ...   \n",
              "2022-06-26            3655204.28  1.802717e+10  17653.31    3228.4   3501.68   \n",
              "2022-06-27            2980504.09  2.096570e+10  17653.31    3228.4   3501.68   \n",
              "2022-06-28            2980504.09  2.138154e+10  17653.31    3228.4   3501.68   \n",
              "2022-06-29            2980504.09  2.355274e+10  17653.31    3228.4   3501.68   \n",
              "2022-06-30            4723524.19  2.626724e+10  17653.31    3228.4   3501.68   \n",
              "\n",
              "              SP500  VXVCLS      DJIA  NetworkVolume      NBI   XAUUSD  \n",
              "Date                                                                    \n",
              "2021-06-29  4291.80   19.49  34292.29          415.0  5129.75  1761.09  \n",
              "2021-06-30  4297.50   19.51  34502.51          292.0  5147.93  1769.80  \n",
              "2021-07-01  4319.94   19.45  34633.53          332.0  5207.87  1776.60  \n",
              "2021-07-02  4352.34   19.30  34786.35          249.0  5184.26  1786.79  \n",
              "2021-07-03  4352.34   19.30  34786.35          247.0  5184.26  1786.79  \n",
              "...             ...     ...       ...            ...      ...      ...  \n",
              "2022-06-26  3911.74   28.95  31500.68          145.0  3849.63  1826.18  \n",
              "2022-06-27  3900.11   28.18  31438.26          245.0  3840.99  1822.73  \n",
              "2022-06-28  3821.55   29.15  30946.99          240.0  3750.31  1819.69  \n",
              "2022-06-29  3818.83   29.00  31029.31          322.0  3778.65  1817.11  \n",
              "2022-06-30  3785.38   29.71  30775.43          286.0  3748.81  1806.89  \n",
              "\n",
              "[362 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07d9ea04-d35b-48bc-9f61-11e4e0173588\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>Average Confirmation Time</th>\n",
              "      <th>Block Height</th>\n",
              "      <th>Difficulty</th>\n",
              "      <th>Miners Revenue (USD)</th>\n",
              "      <th>Unique Addresses Used</th>\n",
              "      <th>Blockchain Size (MB)</th>\n",
              "      <th>Output Value Per Day</th>\n",
              "      <th>Volume</th>\n",
              "      <th>RPI</th>\n",
              "      <th>TOTRESNS</th>\n",
              "      <th>NONREVSL</th>\n",
              "      <th>SP500</th>\n",
              "      <th>VXVCLS</th>\n",
              "      <th>DJIA</th>\n",
              "      <th>NetworkVolume</th>\n",
              "      <th>NBI</th>\n",
              "      <th>XAUUSD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2021-06-29</th>\n",
              "      <td>41564.36</td>\n",
              "      <td>168.03</td>\n",
              "      <td>687456.0</td>\n",
              "      <td>1.990000e+13</td>\n",
              "      <td>20581265.56</td>\n",
              "      <td>379066.0</td>\n",
              "      <td>352241.73</td>\n",
              "      <td>1376609.20</td>\n",
              "      <td>3.790146e+10</td>\n",
              "      <td>17827.22</td>\n",
              "      <td>3848.1</td>\n",
              "      <td>3308.21</td>\n",
              "      <td>4291.80</td>\n",
              "      <td>19.49</td>\n",
              "      <td>34292.29</td>\n",
              "      <td>415.0</td>\n",
              "      <td>5129.75</td>\n",
              "      <td>1761.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-06-30</th>\n",
              "      <td>43790.89</td>\n",
              "      <td>168.03</td>\n",
              "      <td>687456.0</td>\n",
              "      <td>1.990000e+13</td>\n",
              "      <td>20581265.56</td>\n",
              "      <td>567655.0</td>\n",
              "      <td>352241.73</td>\n",
              "      <td>1376609.20</td>\n",
              "      <td>3.405904e+10</td>\n",
              "      <td>17827.22</td>\n",
              "      <td>3848.1</td>\n",
              "      <td>3308.21</td>\n",
              "      <td>4297.50</td>\n",
              "      <td>19.51</td>\n",
              "      <td>34502.51</td>\n",
              "      <td>292.0</td>\n",
              "      <td>5147.93</td>\n",
              "      <td>1769.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-01</th>\n",
              "      <td>48116.94</td>\n",
              "      <td>168.03</td>\n",
              "      <td>687456.0</td>\n",
              "      <td>1.990000e+13</td>\n",
              "      <td>22482193.60</td>\n",
              "      <td>567655.0</td>\n",
              "      <td>352608.29</td>\n",
              "      <td>1376609.20</td>\n",
              "      <td>3.783896e+10</td>\n",
              "      <td>17985.18</td>\n",
              "      <td>3943.9</td>\n",
              "      <td>3318.74</td>\n",
              "      <td>4319.94</td>\n",
              "      <td>19.45</td>\n",
              "      <td>34633.53</td>\n",
              "      <td>332.0</td>\n",
              "      <td>5207.87</td>\n",
              "      <td>1776.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-02</th>\n",
              "      <td>47711.49</td>\n",
              "      <td>191.55</td>\n",
              "      <td>687456.0</td>\n",
              "      <td>1.990000e+13</td>\n",
              "      <td>22482193.60</td>\n",
              "      <td>567655.0</td>\n",
              "      <td>352608.29</td>\n",
              "      <td>1732809.50</td>\n",
              "      <td>3.872897e+10</td>\n",
              "      <td>17985.18</td>\n",
              "      <td>3943.9</td>\n",
              "      <td>3318.74</td>\n",
              "      <td>4352.34</td>\n",
              "      <td>19.30</td>\n",
              "      <td>34786.35</td>\n",
              "      <td>249.0</td>\n",
              "      <td>5184.26</td>\n",
              "      <td>1786.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-07-03</th>\n",
              "      <td>48199.95</td>\n",
              "      <td>191.55</td>\n",
              "      <td>689472.0</td>\n",
              "      <td>1.990000e+13</td>\n",
              "      <td>22482193.60</td>\n",
              "      <td>583136.0</td>\n",
              "      <td>352608.29</td>\n",
              "      <td>1732809.50</td>\n",
              "      <td>2.438396e+10</td>\n",
              "      <td>17985.18</td>\n",
              "      <td>3943.9</td>\n",
              "      <td>3318.74</td>\n",
              "      <td>4352.34</td>\n",
              "      <td>19.30</td>\n",
              "      <td>34786.35</td>\n",
              "      <td>247.0</td>\n",
              "      <td>5184.26</td>\n",
              "      <td>1786.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-06-26</th>\n",
              "      <td>19225.70</td>\n",
              "      <td>28.97</td>\n",
              "      <td>741888.0</td>\n",
              "      <td>2.960000e+13</td>\n",
              "      <td>18725062.51</td>\n",
              "      <td>557791.0</td>\n",
              "      <td>413407.51</td>\n",
              "      <td>3655204.28</td>\n",
              "      <td>1.802717e+10</td>\n",
              "      <td>17653.31</td>\n",
              "      <td>3228.4</td>\n",
              "      <td>3501.68</td>\n",
              "      <td>3911.74</td>\n",
              "      <td>28.95</td>\n",
              "      <td>31500.68</td>\n",
              "      <td>145.0</td>\n",
              "      <td>3849.63</td>\n",
              "      <td>1826.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-06-27</th>\n",
              "      <td>19081.00</td>\n",
              "      <td>128.29</td>\n",
              "      <td>741888.0</td>\n",
              "      <td>2.960000e+13</td>\n",
              "      <td>18725062.51</td>\n",
              "      <td>557791.0</td>\n",
              "      <td>413407.51</td>\n",
              "      <td>2980504.09</td>\n",
              "      <td>2.096570e+10</td>\n",
              "      <td>17653.31</td>\n",
              "      <td>3228.4</td>\n",
              "      <td>3501.68</td>\n",
              "      <td>3900.11</td>\n",
              "      <td>28.18</td>\n",
              "      <td>31438.26</td>\n",
              "      <td>245.0</td>\n",
              "      <td>3840.99</td>\n",
              "      <td>1822.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-06-28</th>\n",
              "      <td>19411.00</td>\n",
              "      <td>128.29</td>\n",
              "      <td>741888.0</td>\n",
              "      <td>2.960000e+13</td>\n",
              "      <td>18725062.51</td>\n",
              "      <td>624229.0</td>\n",
              "      <td>413407.51</td>\n",
              "      <td>2980504.09</td>\n",
              "      <td>2.138154e+10</td>\n",
              "      <td>17653.31</td>\n",
              "      <td>3228.4</td>\n",
              "      <td>3501.68</td>\n",
              "      <td>3821.55</td>\n",
              "      <td>29.15</td>\n",
              "      <td>30946.99</td>\n",
              "      <td>240.0</td>\n",
              "      <td>3750.31</td>\n",
              "      <td>1819.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-06-29</th>\n",
              "      <td>19593.00</td>\n",
              "      <td>128.29</td>\n",
              "      <td>741888.0</td>\n",
              "      <td>2.960000e+13</td>\n",
              "      <td>19294230.34</td>\n",
              "      <td>624229.0</td>\n",
              "      <td>413947.56</td>\n",
              "      <td>2980504.09</td>\n",
              "      <td>2.355274e+10</td>\n",
              "      <td>17653.31</td>\n",
              "      <td>3228.4</td>\n",
              "      <td>3501.68</td>\n",
              "      <td>3818.83</td>\n",
              "      <td>29.00</td>\n",
              "      <td>31029.31</td>\n",
              "      <td>322.0</td>\n",
              "      <td>3778.65</td>\n",
              "      <td>1817.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-06-30</th>\n",
              "      <td>19423.00</td>\n",
              "      <td>19.40</td>\n",
              "      <td>741888.0</td>\n",
              "      <td>2.960000e+13</td>\n",
              "      <td>19294230.34</td>\n",
              "      <td>624229.0</td>\n",
              "      <td>413947.56</td>\n",
              "      <td>4723524.19</td>\n",
              "      <td>2.626724e+10</td>\n",
              "      <td>17653.31</td>\n",
              "      <td>3228.4</td>\n",
              "      <td>3501.68</td>\n",
              "      <td>3785.38</td>\n",
              "      <td>29.71</td>\n",
              "      <td>30775.43</td>\n",
              "      <td>286.0</td>\n",
              "      <td>3748.81</td>\n",
              "      <td>1806.89</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>362 rows × 18 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07d9ea04-d35b-48bc-9f61-11e4e0173588')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07d9ea04-d35b-48bc-9f61-11e4e0173588 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07d9ea04-d35b-48bc-9f61-11e4e0173588');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/',force_remount=True)\n",
        "#%cd /content/drive/My-Drive/\n",
        "# 在線GPU性能检验\n",
        "!nvidia-smi \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/',force_remount=True)\n",
        "#%cd /content/drive/My-Drive/\n",
        "# 在線GPU性能检验\n",
        "!nvidia-smi \n",
        "raw = pd.read_csv('/content/gdrive/MyDrive/科技部計畫/data/變數預測股價/DayForQuarter.csv',index_col=[0],parse_dates=[\"Date\"])\n",
        "\n",
        "#read the data\n",
        "stock = raw \n",
        "stock = stock.loc[:,[\"Close\",'Average Confirmation Time',\"Block Height\",'Difficulty','Miners Revenue (USD)','Unique Addresses Used','Blockchain Size (MB)','Output Value Per Day','Volume','RPI','TOTRESNS','NONREVSL','SP500','VXVCLS','DJIA','NetworkVolume','NBI','XAUUSD']]\n",
        "amount = len(stock) \n",
        "stock = stock['2017-01-01':'2022-07-30']\n",
        "stock[1619:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we86jPPbJKjn",
        "outputId": "7a260739-530e-444a-9d28-176bc5d87569"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1981, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# lcd[~(lcd['收盤價-除權息'])]\n",
        "y = stock[\"Close\"]\n",
        "x = stock\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq9NIG-4LzTZ"
      },
      "outputs": [],
      "source": [
        "#將資料正規化，讓資料變成0~1之間，讓資料訓練速度更快，且更容易收斂\n",
        "\n",
        "y= y.values.reshape(-1,1) \n",
        "scaler=MinMaxScaler(feature_range=(0,1))\n",
        "scaler1=MinMaxScaler(feature_range=(0,1))\n",
        "y=scaler.fit_transform(y)\n",
        "x=scaler1.fit_transform(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jyc0iHWL-nF"
      },
      "outputs": [],
      "source": [
        "#並將資料分成訓練組50，驗證組25，測試組25\n",
        "#def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
        "#                      target_size, single_step=False):\n",
        "x,y=multivariate_data( x ,y , 0 ,None, 1 , 1 ,single_step=True)\n",
        "i=1619\n",
        "x_train,y_train  = x[0:int(i)] , y[0:int(i)]\n",
        "x_vaild,y_vaild  = x[int(i*2/3):int(i)] , y[int(i*2/3):int(i)]\n",
        "x_test ,y_test   = x[int(i):-1] , y[int(i):-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WQrZknPMKok"
      },
      "outputs": [],
      "source": [
        "trainindex= stock.index[10:len(x_train)+10]\n",
        "valindex = stock.index[len(x_train)+10:len(x_train)+10+len(x_vaild)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu382YOGMqne",
        "outputId": "df150d0e-1701-4793-d1bb-9ef42a7022cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1619, 1, 18) (1619, 1) (540, 1, 18) (540, 1) (359, 1, 18) (359, 1)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape,y_train.shape,x_vaild.shape,y_vaild.shape,x_test.shape ,y_test.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YxFX1ToHbFt"
      },
      "outputs": [],
      "source": [
        "##LSTM\n",
        "def lstm_model(input_length, input_dim):\n",
        "\n",
        "    d=0.5\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=80, activation=\"relu\", return_sequences=False, input_shape=(input_length, input_dim)))\n",
        "    model.add(Dropout(d))  \n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    # optimizer = tf.keras.optimizers.Adam(lr=0.00005)\n",
        "    model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['mean_squared_error'])#loss=mse/categorical_crossentropy\n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FcUvWEuMsX6",
        "outputId": "79d484bf-4520-4a89-8d4d-56c953cceb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0047 - mean_squared_error: 0.0047\n",
            "Epoch 1: val_loss improved from inf to 0.06516, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 4s 30ms/step - loss: 0.0321 - mean_squared_error: 0.0321 - val_loss: 0.0652 - val_mean_squared_error: 0.0652\n",
            "Epoch 2/100\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110\n",
            "Epoch 2: val_loss improved from 0.06516 to 0.03863, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0386 - val_mean_squared_error: 0.0386\n",
            "Epoch 3/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.0203\n",
            "Epoch 3: val_loss improved from 0.03863 to 0.03386, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0339 - val_mean_squared_error: 0.0339\n",
            "Epoch 4/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.0164\n",
            "Epoch 4: val_loss improved from 0.03386 to 0.02935, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0293 - val_mean_squared_error: 0.0293\n",
            "Epoch 5/100\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0062 - mean_squared_error: 0.0062\n",
            "Epoch 5: val_loss improved from 0.02935 to 0.02585, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0141 - mean_squared_error: 0.0141 - val_loss: 0.0259 - val_mean_squared_error: 0.0259\n",
            "Epoch 6/100\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0122 - mean_squared_error: 0.0122\n",
            "Epoch 6: val_loss improved from 0.02585 to 0.02216, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0124 - mean_squared_error: 0.0124 - val_loss: 0.0222 - val_mean_squared_error: 0.0222\n",
            "Epoch 7/100\n",
            "21/26 [=======================>......] - ETA: 0s - loss: 0.0041 - mean_squared_error: 0.0041\n",
            "Epoch 7: val_loss improved from 0.02216 to 0.02002, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0105 - mean_squared_error: 0.0105 - val_loss: 0.0200 - val_mean_squared_error: 0.0200\n",
            "Epoch 8/100\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0066 - mean_squared_error: 0.0066\n",
            "Epoch 8: val_loss improved from 0.02002 to 0.01798, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
            "Epoch 9/100\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0024 - mean_squared_error: 0.0024\n",
            "Epoch 9: val_loss improved from 0.01798 to 0.01649, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 11ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
            "Epoch 10/100\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0074 - mean_squared_error: 0.0074\n",
            "Epoch 10: val_loss improved from 0.01649 to 0.01511, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0081 - mean_squared_error: 0.0081 - val_loss: 0.0151 - val_mean_squared_error: 0.0151\n",
            "Epoch 11/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0077 - mean_squared_error: 0.0077\n",
            "Epoch 11: val_loss improved from 0.01511 to 0.01384, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0138 - val_mean_squared_error: 0.0138\n",
            "Epoch 12/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
            "Epoch 12: val_loss improved from 0.01384 to 0.01294, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0129 - val_mean_squared_error: 0.0129\n",
            "Epoch 13/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0065 - mean_squared_error: 0.0065\n",
            "Epoch 13: val_loss improved from 0.01294 to 0.01203, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0120 - val_mean_squared_error: 0.0120\n",
            "Epoch 14/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0059 - mean_squared_error: 0.0059\n",
            "Epoch 14: val_loss improved from 0.01203 to 0.01117, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
            "Epoch 15/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0058 - mean_squared_error: 0.0058\n",
            "Epoch 15: val_loss improved from 0.01117 to 0.01012, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 14ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
            "Epoch 16/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0052 - mean_squared_error: 0.0052\n",
            "Epoch 16: val_loss improved from 0.01012 to 0.00941, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0094 - val_mean_squared_error: 0.0094\n",
            "Epoch 17/100\n",
            "23/26 [=========================>....] - ETA: 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
            "Epoch 17: val_loss improved from 0.00941 to 0.00879, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 18ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
            "Epoch 18/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0051 - mean_squared_error: 0.0051\n",
            "Epoch 18: val_loss improved from 0.00879 to 0.00795, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 11ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
            "Epoch 19/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
            "Epoch 19: val_loss improved from 0.00795 to 0.00728, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 14ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0073 - val_mean_squared_error: 0.0073\n",
            "Epoch 20/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0045 - mean_squared_error: 0.0045\n",
            "Epoch 20: val_loss improved from 0.00728 to 0.00663, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0066 - val_mean_squared_error: 0.0066\n",
            "Epoch 21/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
            "Epoch 21: val_loss improved from 0.00663 to 0.00621, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0062 - val_mean_squared_error: 0.0062\n",
            "Epoch 22/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0042 - mean_squared_error: 0.0042\n",
            "Epoch 22: val_loss improved from 0.00621 to 0.00605, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0042 - mean_squared_error: 0.0042 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
            "Epoch 23/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0037 - mean_squared_error: 0.0037\n",
            "Epoch 23: val_loss improved from 0.00605 to 0.00549, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 12ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
            "Epoch 24/100\n",
            "20/26 [======================>.......] - ETA: 0s - loss: 0.0021 - mean_squared_error: 0.0021\n",
            "Epoch 24: val_loss did not improve from 0.00549\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0058 - val_mean_squared_error: 0.0058\n",
            "Epoch 25/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0043 - mean_squared_error: 0.0043\n",
            "Epoch 25: val_loss improved from 0.00549 to 0.00475, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 15ms/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0048 - val_mean_squared_error: 0.0048\n",
            "Epoch 26/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0036 - mean_squared_error: 0.0036\n",
            "Epoch 26: val_loss improved from 0.00475 to 0.00430, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 11ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0043 - val_mean_squared_error: 0.0043\n",
            "Epoch 27/100\n",
            "19/26 [====================>.........] - ETA: 0s - loss: 0.0014 - mean_squared_error: 0.0014        \n",
            "Epoch 27: val_loss improved from 0.00430 to 0.00415, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 11ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
            "Epoch 28/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0036 - mean_squared_error: 0.0036\n",
            "Epoch 28: val_loss improved from 0.00415 to 0.00378, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 13ms/step - loss: 0.0036 - mean_squared_error: 0.0036 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
            "Epoch 29/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0033 - mean_squared_error: 0.0033\n",
            "Epoch 29: val_loss improved from 0.00378 to 0.00347, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 11ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
            "Epoch 30/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        \n",
            "Epoch 30: val_loss improved from 0.00347 to 0.00324, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "Epoch 31/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        \n",
            "Epoch 31: val_loss did not improve from 0.00324\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "Epoch 32/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0033 - mean_squared_error: 0.0033        \n",
            "Epoch 32: val_loss improved from 0.00324 to 0.00306, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "Epoch 33/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 9.4427e-04 - mean_squared_error: 9.4427e-04\n",
            "Epoch 33: val_loss improved from 0.00306 to 0.00262, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "Epoch 34/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 9.7736e-04 - mean_squared_error: 9.7736e-04\n",
            "Epoch 34: val_loss improved from 0.00262 to 0.00251, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "Epoch 35/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0013 - mean_squared_error: 0.0013        \n",
            "Epoch 35: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
            "Epoch 36/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 9.0739e-04 - mean_squared_error: 9.0739e-04\n",
            "Epoch 36: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "Epoch 37/100\n",
            "17/26 [==================>...........] - ETA: 0s - loss: 0.0013 - mean_squared_error: 0.0013        \n",
            "Epoch 37: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "Epoch 38/100\n",
            "17/26 [==================>...........] - ETA: 0s - loss: 0.0012 - mean_squared_error: 0.0012        \n",
            "Epoch 38: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "Epoch 39/100\n",
            "17/26 [==================>...........] - ETA: 0s - loss: 0.0023 - mean_squared_error: 0.0023        \n",
            "Epoch 39: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
            "Epoch 40/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0017 - mean_squared_error: 0.0017\n",
            "Epoch 40: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n",
            "Epoch 41/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0030 - mean_squared_error: 0.0030\n",
            "Epoch 41: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
            "Epoch 42/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0027 - mean_squared_error: 0.0027        \n",
            "Epoch 42: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "Epoch 43/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 0.0019 - mean_squared_error: 0.0019        \n",
            "Epoch 43: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
            "Epoch 44/100\n",
            "16/26 [=================>............] - ETA: 0s - loss: 0.0034 - mean_squared_error: 0.0034\n",
            "Epoch 44: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
            "Epoch 45/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 9.0243e-04 - mean_squared_error: 9.0243e-04\n",
            "Epoch 45: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
            "Epoch 46/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0044 - mean_squared_error: 0.0044\n",
            "Epoch 46: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "Epoch 47/100\n",
            "18/26 [===================>..........] - ETA: 0s - loss: 8.8186e-04 - mean_squared_error: 8.8186e-04\n",
            "Epoch 47: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
            "Epoch 48/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 0.0025 - mean_squared_error: 0.0025\n",
            "Epoch 48: val_loss did not improve from 0.00251\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "Epoch 49/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 7.5444e-04 - mean_squared_error: 7.5444e-04\n",
            "Epoch 49: val_loss improved from 0.00251 to 0.00242, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "Epoch 50/100\n",
            "16/26 [=================>............] - ETA: 0s - loss: 0.0025 - mean_squared_error: 0.0025        \n",
            "Epoch 50: val_loss did not improve from 0.00242\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0039 - mean_squared_error: 0.0039 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
            "Epoch 51/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0012 - mean_squared_error: 0.0012\n",
            "Epoch 51: val_loss did not improve from 0.00242\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
            "Epoch 52/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0034 - mean_squared_error: 0.0034\n",
            "Epoch 52: val_loss did not improve from 0.00242\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
            "Epoch 53/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0031 - mean_squared_error: 0.0031\n",
            "Epoch 53: val_loss improved from 0.00242 to 0.00190, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "Epoch 54/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 9.9502e-04 - mean_squared_error: 9.9502e-04\n",
            "Epoch 54: val_loss did not improve from 0.00190\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0040 - val_mean_squared_error: 0.0040\n",
            "Epoch 55/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0016 - mean_squared_error: 0.0016\n",
            "Epoch 55: val_loss did not improve from 0.00190\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "Epoch 56/100\n",
            "16/26 [=================>............] - ETA: 0s - loss: 5.9520e-04 - mean_squared_error: 5.9520e-04\n",
            "Epoch 56: val_loss did not improve from 0.00190\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "Epoch 57/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0010 - mean_squared_error: 0.0010        \n",
            "Epoch 57: val_loss improved from 0.00190 to 0.00156, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "Epoch 58/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 8.0327e-04 - mean_squared_error: 8.0327e-04\n",
            "Epoch 58: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "Epoch 59/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0022 - mean_squared_error: 0.0022        \n",
            "Epoch 59: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "Epoch 60/100\n",
            "18/26 [===================>..........] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        \n",
            "Epoch 60: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "Epoch 61/100\n",
            "16/26 [=================>............] - ETA: 0s - loss: 7.3954e-04 - mean_squared_error: 7.3954e-04\n",
            "Epoch 61: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "Epoch 62/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0017 - mean_squared_error: 0.0017        \n",
            "Epoch 62: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0039 - val_mean_squared_error: 0.0039\n",
            "Epoch 63/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0016 - mean_squared_error: 0.0016\n",
            "Epoch 63: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0033 - mean_squared_error: 0.0033 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "Epoch 64/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0028 - mean_squared_error: 0.0028        \n",
            "Epoch 64: val_loss did not improve from 0.00156\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "Epoch 65/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0015 - mean_squared_error: 0.0015        \n",
            "Epoch 65: val_loss improved from 0.00156 to 0.00146, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 66/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0030 - mean_squared_error: 0.0030        \n",
            "Epoch 66: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0031 - mean_squared_error: 0.0031 - val_loss: 0.0055 - val_mean_squared_error: 0.0055\n",
            "Epoch 67/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0049 - mean_squared_error: 0.0049\n",
            "Epoch 67: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "Epoch 68/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 6.4088e-04 - mean_squared_error: 6.4088e-04\n",
            "Epoch 68: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0045 - val_mean_squared_error: 0.0045\n",
            "Epoch 69/100\n",
            "16/26 [=================>............] - ETA: 0s - loss: 0.0028 - mean_squared_error: 0.0028        \n",
            "Epoch 69: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0038 - mean_squared_error: 0.0038 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
            "Epoch 70/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 7.0223e-04 - mean_squared_error: 7.0223e-04\n",
            "Epoch 70: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
            "Epoch 71/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0022 - mean_squared_error: 0.0022\n",
            "Epoch 71: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0037 - mean_squared_error: 0.0037 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "Epoch 72/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        \n",
            "Epoch 72: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 73/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 0.0010 - mean_squared_error: 0.0010        \n",
            "Epoch 73: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "Epoch 74/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 0.0015 - mean_squared_error: 0.0015\n",
            "Epoch 74: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0029 - mean_squared_error: 0.0029 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "Epoch 75/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0018 - mean_squared_error: 0.0018        \n",
            "Epoch 75: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "Epoch 76/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        \n",
            "Epoch 76: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 77/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 5.9843e-04 - mean_squared_error: 5.9843e-04\n",
            "Epoch 77: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
            "Epoch 78/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0012 - mean_squared_error: 0.0012\n",
            "Epoch 78: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 79/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 6.2410e-04 - mean_squared_error: 6.2410e-04\n",
            "Epoch 79: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "Epoch 80/100\n",
            "16/26 [=================>............] - ETA: 0s - loss: 0.0013 - mean_squared_error: 0.0013        \n",
            "Epoch 80: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
            "Epoch 81/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011\n",
            "Epoch 81: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "Epoch 82/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 8.5122e-04 - mean_squared_error: 8.5122e-04\n",
            "Epoch 82: val_loss did not improve from 0.00146\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "Epoch 83/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0015 - mean_squared_error: 0.0015        \n",
            "Epoch 83: val_loss improved from 0.00146 to 0.00118, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0012 - val_mean_squared_error: 0.0012\n",
            "Epoch 84/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0022 - mean_squared_error: 0.0022        \n",
            "Epoch 84: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
            "Epoch 85/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0032 - mean_squared_error: 0.0032\n",
            "Epoch 85: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
            "Epoch 86/100\n",
            "22/26 [========================>.....] - ETA: 0s - loss: 0.0010 - mean_squared_error: 0.0010        \n",
            "Epoch 86: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
            "Epoch 87/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0034 - mean_squared_error: 0.0034\n",
            "Epoch 87: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0034 - mean_squared_error: 0.0034 - val_loss: 0.0052 - val_mean_squared_error: 0.0052\n",
            "Epoch 88/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0040 - mean_squared_error: 0.0040\n",
            "Epoch 88: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "Epoch 89/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 4.9277e-04 - mean_squared_error: 4.9277e-04\n",
            "Epoch 89: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
            "Epoch 90/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 0.0016 - mean_squared_error: 0.0016        \n",
            "Epoch 90: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0027 - mean_squared_error: 0.0027 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
            "Epoch 91/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0022 - mean_squared_error: 0.0022        \n",
            "Epoch 91: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
            "Epoch 92/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0027 - mean_squared_error: 0.0027\n",
            "Epoch 92: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
            "Epoch 93/100\n",
            "14/26 [===============>..............] - ETA: 0s - loss: 7.6361e-04 - mean_squared_error: 7.6361e-04\n",
            "Epoch 93: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
            "Epoch 94/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0026 - mean_squared_error: 0.0026\n",
            "Epoch 94: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
            "Epoch 95/100\n",
            "12/26 [============>.................] - ETA: 0s - loss: 0.0021 - mean_squared_error: 0.0021\n",
            "Epoch 95: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "Epoch 96/100\n",
            "15/26 [================>.............] - ETA: 0s - loss: 4.2690e-04 - mean_squared_error: 4.2690e-04\n",
            "Epoch 96: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
            "Epoch 97/100\n",
            "13/26 [==============>...............] - ETA: 0s - loss: 0.0011 - mean_squared_error: 0.0011        \n",
            "Epoch 97: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
            "Epoch 98/100\n",
            "24/26 [==========================>...] - ETA: 0s - loss: 0.0018 - mean_squared_error: 0.0018        \n",
            "Epoch 98: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
            "Epoch 99/100\n",
            "25/26 [===========================>..] - ETA: 0s - loss: 0.0020 - mean_squared_error: 0.0020        \n",
            "Epoch 99: val_loss did not improve from 0.00118\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
            "Epoch 100/100\n",
            "26/26 [==============================] - ETA: 0s - loss: 0.0019 - mean_squared_error: 0.0019        \n",
            "Epoch 100: val_loss improved from 0.00118 to 0.00101, saving model to lstm.best.hdf5\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0010 - val_mean_squared_error: 0.0010\n"
          ]
        }
      ],
      "source": [
        "##build LSTM model\n",
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=70, monitor = 'val_loss')\n",
        "    ]######## 在訓練組訓練，使用驗證組選取\n",
        "#EarlyStopping : 在訓練組訓練參數，以驗證組最低為選擇標準，如果300個epochs，沒有改善即停止訓練\n",
        "filepath=\"lstm.best.hdf5\" #模型儲存路徑\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, mode='min',save_best_only=True)\n",
        "#選擇val_loss最低的當作最後的模型\n",
        "call_backlist = [ my_callbacks,checkpoint]\n",
        "callbacks=call_backlist\n",
        "lstm = lstm_model(1,18)\n",
        "historylstm = lstm.fit( x_train, y_train, batch_size=64,shuffle=False , epochs=100,validation_data=(x_vaild,y_vaild),callbacks=call_backlist)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Essu_Kn7MspR",
        "outputId": "4f32af65-1735-43f5-8ce6-30c6573880cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[40934.28 ],\n",
              "       [44410.805],\n",
              "       [48423.836],\n",
              "       [48178.047],\n",
              "       [48642.027],\n",
              "       [49272.17 ],\n",
              "       [51720.805],\n",
              "       [54787.277],\n",
              "       [53202.727],\n",
              "       [53805.57 ],\n",
              "       [54716.66 ],\n",
              "       [54303.344],\n",
              "       [55748.58 ],\n",
              "       [54491.027],\n",
              "       [55662.61 ],\n",
              "       [55447.723],\n",
              "       [59110.438],\n",
              "       [58480.797],\n",
              "       [58939.664],\n",
              "       [57825.2  ],\n",
              "       [59688.598],\n",
              "       [61276.234],\n",
              "       [60061.51 ],\n",
              "       [59045.777],\n",
              "       [59400.203],\n",
              "       [58208.41 ],\n",
              "       [59679.15 ],\n",
              "       [58101.73 ],\n",
              "       [57411.31 ],\n",
              "       [59116.15 ],\n",
              "       [60511.96 ],\n",
              "       [60313.293],\n",
              "       [59682.14 ],\n",
              "       [60054.145],\n",
              "       [61526.13 ],\n",
              "       [61585.1  ],\n",
              "       [60272.23 ],\n",
              "       [60629.13 ],\n",
              "       [60172.48 ],\n",
              "       [61359.977],\n",
              "       [63372.91 ],\n",
              "       [63060.07 ],\n",
              "       [61211.36 ],\n",
              "       [62331.406],\n",
              "       [61913.1  ],\n",
              "       [62320.496],\n",
              "       [61923.68 ],\n",
              "       [59906.02 ],\n",
              "       [57293.656],\n",
              "       [58865.76 ],\n",
              "       [56651.035],\n",
              "       [57221.18 ],\n",
              "       [59244.426],\n",
              "       [57752.496],\n",
              "       [55284.215],\n",
              "       [57297.645],\n",
              "       [56284.758],\n",
              "       [57833.457],\n",
              "       [54514.13 ],\n",
              "       [56110.438],\n",
              "       [57693.52 ],\n",
              "       [56654.582],\n",
              "       [56055.727],\n",
              "       [58817.863],\n",
              "       [58049.98 ],\n",
              "       [55662.87 ],\n",
              "       [50766.43 ],\n",
              "       [50634.11 ],\n",
              "       [51942.312],\n",
              "       [52867.04 ],\n",
              "       [53048.215],\n",
              "       [50402.824],\n",
              "       [49561.664],\n",
              "       [51315.246],\n",
              "       [52113.496],\n",
              "       [48963.1  ],\n",
              "       [48685.86 ],\n",
              "       [50357.496],\n",
              "       [49505.332],\n",
              "       [48550.656],\n",
              "       [48604.49 ],\n",
              "       [46857.984],\n",
              "       [47047.047],\n",
              "       [52239.207],\n",
              "       [53015.863],\n",
              "       [54759.703],\n",
              "       [52119.902],\n",
              "       [50733.94 ],\n",
              "       [51130.3  ],\n",
              "       [51002.625],\n",
              "       [48429.82 ],\n",
              "       [47388.99 ],\n",
              "       [48496.79 ],\n",
              "       [49308.78 ],\n",
              "       [48070.066],\n",
              "       [46956.49 ],\n",
              "       [46027.035],\n",
              "       [44230.26 ],\n",
              "       [44200.89 ],\n",
              "       [43416.965],\n",
              "       [43876.21 ],\n",
              "       [43495.54 ],\n",
              "       [41688.223],\n",
              "       [42118.695],\n",
              "       [43468.562],\n",
              "       [43743.324],\n",
              "       [44436.195],\n",
              "       [44325.566],\n",
              "       [44139.258],\n",
              "       [42884.754],\n",
              "       [42951.145],\n",
              "       [42944.6  ],\n",
              "       [42034.7  ],\n",
              "       [38025.906],\n",
              "       [36555.207],\n",
              "       [37895.406],\n",
              "       [37764.938],\n",
              "       [38398.695],\n",
              "       [38037.504],\n",
              "       [38255.055],\n",
              "       [39318.61 ],\n",
              "       [39069.797],\n",
              "       [38783.805],\n",
              "       [38541.938],\n",
              "       [39660.79 ],\n",
              "       [37874.523],\n",
              "       [38643.2  ],\n",
              "       [43017.72 ],\n",
              "       [43158.37 ],\n",
              "       [43421.824],\n",
              "       [44504.93 ],\n",
              "       [45649.277],\n",
              "       [46887.566],\n",
              "       [46043.746],\n",
              "       [44249.71 ],\n",
              "       [43570.227],\n",
              "       [43460.887],\n",
              "       [43483.438],\n",
              "       [45464.844],\n",
              "       [44956.887],\n",
              "       [42771.812],\n",
              "       [42188.777],\n",
              "       [42472.79 ],\n",
              "       [39839.38 ],\n",
              "       [37928.062],\n",
              "       [38665.26 ],\n",
              "       [37772.01 ],\n",
              "       [39420.836],\n",
              "       [40395.55 ],\n",
              "       [40849.664],\n",
              "       [38892.805],\n",
              "       [44070.33 ],\n",
              "       [45263.918],\n",
              "       [44659.24 ],\n",
              "       [43279.555],\n",
              "       [40227.367],\n",
              "       [40016.7  ],\n",
              "       [38575.547],\n",
              "       [38503.355],\n",
              "       [39627.52 ],\n",
              "       [42508.13 ],\n",
              "       [39988.473],\n",
              "       [38583.863],\n",
              "       [37968.332],\n",
              "       [37517.1  ],\n",
              "       [40793.086],\n",
              "       [41526.707],\n",
              "       [43704.957],\n",
              "       [42153.793],\n",
              "       [42456.824],\n",
              "       [42452.477],\n",
              "       [41546.95 ],\n",
              "       [41938.008],\n",
              "       [43204.047],\n",
              "       [44003.695],\n",
              "       [44627.32 ],\n",
              "       [44816.176],\n",
              "       [44691.344],\n",
              "       [46535.03 ],\n",
              "       [46760.82 ],\n",
              "       [47611.516],\n",
              "       [47451.34 ],\n",
              "       [46117.66 ],\n",
              "       [45776.793],\n",
              "       [44202.055],\n",
              "       [44688.58 ],\n",
              "       [45000.203],\n",
              "       [44979.32 ],\n",
              "       [42866.17 ],\n",
              "       [43444.715],\n",
              "       [41187.742],\n",
              "       [41547.598],\n",
              "       [41454.79 ],\n",
              "       [39865.84 ],\n",
              "       [40288.58 ],\n",
              "       [41415.43 ],\n",
              "       [40072.184],\n",
              "       [40270.203],\n",
              "       [39961.043],\n",
              "       [39251.39 ],\n",
              "       [40160.32 ],\n",
              "       [40911.266],\n",
              "       [40963.125],\n",
              "       [40427.26 ],\n",
              "       [40094.758],\n",
              "       [38974.527],\n",
              "       [39344.266],\n",
              "       [39844.586],\n",
              "       [37785.918],\n",
              "       [38459.766],\n",
              "       [38705.004],\n",
              "       [37141.19 ],\n",
              "       [36147.68 ],\n",
              "       [37590.02 ],\n",
              "       [37482.297],\n",
              "       [36468.59 ],\n",
              "       [38274.133],\n",
              "       [35430.715],\n",
              "       [34718.594],\n",
              "       [34742.547],\n",
              "       [33128.082],\n",
              "       [30316.58 ],\n",
              "       [30823.729],\n",
              "       [29132.047],\n",
              "       [28519.207],\n",
              "       [28431.72 ],\n",
              "       [29381.852],\n",
              "       [30495.15 ],\n",
              "       [29519.674],\n",
              "       [30273.096],\n",
              "       [28439.559],\n",
              "       [29408.246],\n",
              "       [28486.762],\n",
              "       [29336.559],\n",
              "       [30345.955],\n",
              "       [29256.393],\n",
              "       [29800.746],\n",
              "       [29568.16 ],\n",
              "       [29092.762],\n",
              "       [28304.277],\n",
              "       [28565.541],\n",
              "       [29820.299],\n",
              "       [30516.52 ],\n",
              "       [29687.549],\n",
              "       [30349.443],\n",
              "       [29456.582],\n",
              "       [30548.69 ],\n",
              "       [30806.045],\n",
              "       [31765.146],\n",
              "       [32242.664],\n",
              "       [31226.   ],\n",
              "       [28590.248],\n",
              "       [26298.906],\n",
              "       [21931.662],\n",
              "       [21764.418],\n",
              "       [22375.547],\n",
              "       [20550.602],\n",
              "       [20870.166],\n",
              "       [19529.299],\n",
              "       [20756.332],\n",
              "       [20150.418],\n",
              "       [20509.428],\n",
              "       [19932.906],\n",
              "       [21262.55 ],\n",
              "       [21721.334],\n",
              "       [21899.049],\n",
              "       [21059.04 ],\n",
              "       [20651.316],\n",
              "       [20141.6  ],\n",
              "       [20480.04 ],\n",
              "       [20264.629],\n",
              "       [19274.916],\n",
              "       [19150.826],\n",
              "       [19012.268],\n",
              "       [20218.16 ],\n",
              "       [20085.   ],\n",
              "       [20745.717],\n",
              "       [22546.188],\n",
              "       [22714.354],\n",
              "       [22336.037],\n",
              "       [20882.656],\n",
              "       [20216.209],\n",
              "       [19673.947],\n",
              "       [21407.111],\n",
              "       [21897.53 ],\n",
              "       [21862.812],\n",
              "       [21293.16 ],\n",
              "       [20614.953],\n",
              "       [22625.854],\n",
              "       [23801.588],\n",
              "       [24095.197],\n",
              "       [23753.006],\n",
              "       [23028.996],\n",
              "       [22389.443],\n",
              "       [22560.281],\n",
              "       [22243.752],\n",
              "       [22066.69 ],\n",
              "       [23533.512],\n",
              "       [23893.045],\n",
              "       [23721.44 ],\n",
              "       [23218.861],\n",
              "       [22225.586],\n",
              "       [21708.277],\n",
              "       [21656.385],\n",
              "       [22381.887],\n",
              "       [23084.531],\n",
              "       [22405.809],\n",
              "       [21506.77 ],\n",
              "       [22159.453],\n",
              "       [21600.975],\n",
              "       [23428.303],\n",
              "       [23342.719],\n",
              "       [23888.951],\n",
              "       [23474.531],\n",
              "       [22840.785],\n",
              "       [22447.1  ],\n",
              "       [22216.23 ],\n",
              "       [21979.7  ],\n",
              "       [21334.748],\n",
              "       [18518.623],\n",
              "       [18502.139],\n",
              "       [18592.621],\n",
              "       [17886.422],\n",
              "       [18451.39 ],\n",
              "       [18184.848],\n",
              "       [18881.178],\n",
              "       [17882.021],\n",
              "       [17842.9  ],\n",
              "       [17159.047],\n",
              "       [17494.19 ],\n",
              "       [17715.873],\n",
              "       [18413.828],\n",
              "       [18562.945],\n",
              "       [18363.299],\n",
              "       [17110.125],\n",
              "       [17060.578],\n",
              "       [16893.49 ],\n",
              "       [16610.766],\n",
              "       [17276.291],\n",
              "       [17295.758],\n",
              "       [18591.77 ],\n",
              "       [18900.875],\n",
              "       [18616.842],\n",
              "       [19476.01 ],\n",
              "       [17805.223],\n",
              "       [18835.846],\n",
              "       [20326.71 ],\n",
              "       [20152.92 ],\n",
              "       [20514.422],\n",
              "       [18725.643],\n",
              "       [19368.19 ],\n",
              "       [19242.932],\n",
              "       [17199.316],\n",
              "       [17328.58 ],\n",
              "       [17450.883],\n",
              "       [17289.691],\n",
              "       [16447.88 ],\n",
              "       [16893.115],\n",
              "       [16949.855]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "##LSTM model predict performace\n",
        "lstm_train  = lstm.predict(x_train)\n",
        "lstm_val = lstm.predict(x_vaild)\n",
        "lstm_pre = lstm.predict(x_test)\n",
        "plt.plot(historylstm.history['loss'])\n",
        "plt.plot(historylstm.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "pre = lstm.predict(x_train)\n",
        "pre1=lstm.predict(x_vaild)\n",
        "fc=np.concatenate((pre,pre1))\n",
        "yreal=np.concatenate((y_train,y_vaild))\n",
        "plt.figure(facecolor='white')\n",
        "pd.Series(fc.reshape(-1)).plot(color='red', label='Predict_LSTM')\n",
        "pd.Series(yreal.reshape(-1)).plot(color='blue', label='Original_LSTM')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "lstm_pre= scaler.inverse_transform(lstm_pre)\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1,1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(lstm_pre)\n",
        "plt.plot(y_test)\n",
        "plt.title('pre')\n",
        "plt.ylabel('股價')\n",
        "plt.xlabel('day')\n",
        "plt.legend(['pre', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "root_mean_squared_error(lstm_pre,y_test) \n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test,lstm_pre)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}